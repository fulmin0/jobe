# Universal Scoring Guide

Synthesized from Dianna Yau's PM interview coaching series. This scale applies across all question types and subtypes.

## Scale Definition

| Score | Label | What It Looks Like |
|-------|-------|--------------------|
| 5 | Exceptional | Interviewer-caliber answer. Novel insights, deeply structured, would strongly pass. Demonstrates the kind of thinking an experienced PM brings daily. Solutions are creative yet feasible, pain points reveal non-obvious user empathy, and strategic reasoning is airtight. |
| 4 | Strong | Clear depth beyond the basics. Creative insights, well-articulated reasoning, objective prioritization with explicit criteria. Candidate drives the conversation without prompting. Minor gaps don't undermine overall quality. |
| 3 | Solid | Covers key points with adequate structure. Follows a framework consistently, identifies relevant users/pain points/solutions, and provides reasonable justification. Some gaps in depth, specificity, or creativity. Would likely pass at most companies. |
| 2 | Below Expectations | Attempts structure but has major omissions. Reasoning is vague or subjective, prioritization lacks clear criteria, and answers feel generic or memorized. Requires significant interviewer prompting to advance. |
| 1 | Significant Gaps | Unstructured response. Misses most key aspects, jumps to solutions without problem definition, uses no framework, or demonstrates fundamental misunderstanding of the question type. Would not pass. |

**Source:** Evaluation patterns across mock interviews (`WdssgqA5-aI`, `EOwkN1gQfsk`, `zR2A14v5Awg`, `PU635pDoxy4`, `dz-hqOih9qY`) and teaching videos (`8QybwDJk0Ek`, `JJfLOSRY1hk`).

## What Interviewers Actually Assess

Two meta-dimensions apply across all question types (from `vpsUw1vP8w4`):

1. **Strategic Thinking** — Big picture, goals, trade-offs, business model reasoning, mission alignment
2. **Product Sense** — User empathy, delightful UX with minimum friction, design choices that drive business goals

> "Interviewers are trying to assess two things: strategic thinking and product sense." — Dianna Yau

## Overall Score Calculation

- Average of all dimension scores, weighted by dimension weights specified in each rubric
- A single dimension score of 1 caps the overall at 3.0 maximum (a critical gap prevents a strong overall rating)

## Passing Thresholds

| Score Range | Assessment |
|-------------|------------|
| Below 2.5 | Needs significant improvement — major structural or depth issues |
| 2.5 – 3.0 | Borderline — risky at top companies, may pass at others |
| 3.0 – 3.5 | Likely pass at most companies |
| 3.5 – 4.0 | Strong pass — demonstrates product leadership thinking |
| 4.0+ | Exceptional — top-tier candidate, could be a bar-raiser |

## Key Signals That Separate Scores

### From 2 → 3: Structure
- Uses a stated framework and follows it consistently
- Identifies at least 3 user segments, pain points, or solutions
- Provides some reasoning for choices (not just listing)

### From 3 → 4: Depth and Objectivity
- Uses **objective criteria** for every prioritization decision (TAM, frequency, severity, existing solutions)
- Shows emotional empathy with users — "you're getting so entrenched into the user shoes that you're complaining as if you were one of the users" (`aDRVDvxnDR8`)
- Drives conversation without interviewer prompting
- Acknowledges existing solutions and builds beyond them

### From 4 → 5: Insight and Creativity
- Reveals **non-obvious insights** through specificity — "the more specific your pain point, the more differentiated your solution" (`KruFg7-9qBM`)
- Combines technologies/trends with user problems in novel ways (`kKNvHDRXgAc`)
- Borrows from other verticals to create cross-pollinated solutions
- Makes the interviewer excited about the solution

## Trend Indicators (for progress tracking)

- **Rising:** Score improved by 0.5+ over last 3 sessions of this type
- **Stable:** Score within 0.5 range over last 3 sessions
- **Declining:** Score dropped by 0.5+ over last 3 sessions

# Execution Evaluation Rubric

Synthesized from 3 Dianna Yau videos covering execution-style PM interview questions. Organized by question subtype.

---

## Dimensions Overview

| # | Dimension | What It Assesses |
|---|-----------|-----------------|
| 1 | Process Rigor | Logical reasoning shown at each step, structured approach |
| 2 | Metrics Quality | North Star selection, counter-metrics, downstream metrics |
| 3 | Data-Driven Thinking | Hypothesis generation, validation approach, experimentation |
| 4 | Prioritization & Trade-offs | Criteria-based decisions, stakeholder considerations |
| 5 | Communication Clarity | Executive summary, structured write-up, conciseness |

---

## Subtype 1: Take-Home Assignments

**Question patterns:** "Develop a product strategy for [Company X]" (written assignment, multi-day)
**Source videos:** `gAeuGjvF7bE`, `rOpYuEOrxPQ`

### Dimension Weights

| Dimension | Weight |
|-----------|--------|
| Process Rigor | 25% |
| Metrics Quality | 15% |
| Data-Driven Thinking | 20% |
| Prioritization & Trade-offs | 20% |
| Communication Clarity | 20% |

Communication Clarity is weighted higher than in live interviews because the deliverable is a written document.

### Scoring Scale

#### 1. Process Rigor (25%)

| Score | Description |
|-------|-------------|
| 5 | Follows the 8-step structure (`gAeuGjvF7bE`): 1) Understand the business (mission, goals, users, business model), 2) Gather data (user research + market research), 3) Identify themes, 4) Prioritize themes, 5) Develop strategy with 2-3 solutions per pillar, 6) Prioritize solutions by ROI, 7) Create wireframes, 8) Build month-by-month roadmap with success metrics + experiments + GTM. |
| 4 | Covers most steps with strong logical flow. Clear reasoning at each stage. |
| 3 | Reasonable structure with most key steps. Some steps are thin. |
| 2 | Partial structure. Jumps from research to solutions without clear reasoning chain. |
| 1 | No visible process. Presents solutions without foundational analysis. |

**Key signal from videos:** "How are you supposed to propose solutions or a strategy if you don't understand the business?" (`gAeuGjvF7bE`). "They don't care about the right answer as much as your process" (`gAeuGjvF7bE`).

#### 2. Metrics Quality (15%)

| Score | Description |
|-------|-------------|
| 5 | Defines success metrics tied to business goals. Proposes specific experiments (A/B tests) for key hypotheses. Includes go-to-market metrics alongside product metrics. |
| 4 | Clear success metrics with experimentation plan. |
| 3 | Identifies metrics but no experimentation plan. |
| 2 | Generic metrics not specific to the strategy. |
| 1 | No metrics or measurement plan. |

#### 3. Data-Driven Thinking (20%)

| Score | Description |
|-------|-------------|
| 5 | Uses both primary research (customer interviews — "nothing beats talking to customers," `gAeuGjvF7bE`) and secondary research (reviews, forums, stats databases, earnings reports). Doesn't go past page 5 on Google — focuses on high-value data. Builds logical conclusions from available data. If data is imperfect, shows clear reasoning chain rather than getting stuck. |
| 4 | Good mix of research types. Logical reasoning from data. |
| 3 | Uses available data but relies more on one type. |
| 2 | Minimal data gathering. Mostly assumptions. |
| 1 | No data used. Pure speculation. |

**Key signal from videos:** "If data is maybe wrong, it matters less than how you built upon that data with logical conclusions" (`gAeuGjvF7bE`).

#### 4. Prioritization & Trade-offs (20%)

| Score | Description |
|-------|-------------|
| 5 | Prioritizes themes using criteria (frequency, impact, urgency). Prioritizes solutions using ROI analysis (effort to build vs. benefit on key goals). Mix of practical solutions and moonshot ideas — "Google looks for moonshots, Facebook looks for practical people problems" (`gAeuGjvF7bE`). Connects strategy back to mission themes. |
| 4 | Clear ROI-based prioritization. Good practical/moonshot balance. |
| 3 | Prioritizes but criteria are implicit. Solutions are all practical or all moonshot. |
| 2 | Lists solutions without prioritizing. |
| 1 | No prioritization. Random assortment. |

#### 5. Communication Clarity (20%)

| Score | Description |
|-------|-------------|
| 5 | Structured written document with executive summary. Wireframes showing key user flows. Month-by-month roadmap that sequences execution. Each section builds logically on the previous. Reader can audit the reasoning at every step. |
| 4 | Well-organized document. Clear structure with visual aids. |
| 3 | Readable but could be better organized. Some sections unclear. |
| 2 | Hard to follow. Missing key sections or logical jumps. |
| 1 | Disorganized wall of text. No structure. |

### Common Mistakes (Take-Home)

- Skipping business understanding and jumping to solutions (`gAeuGjvF7bE`)
- Spending weeks on data gathering / falling down rabbit holes (`gAeuGjvF7bE`)
- Digging past page 5 on Google searches — diminishing returns (`gAeuGjvF7bE`)
- Only showing practical solutions without moonshot thinking or vice versa (`gAeuGjvF7bE`)
- Missing the go-to-market component of execution (`gAeuGjvF7bE`)
- Not connecting strategy back to company mission and goals (`gAeuGjvF7bE`)
- Worrying too much about "wrong" data rather than showing logical reasoning (`gAeuGjvF7bE`)

### Follow-up Probes (Presentation Day)

- "Walk me through how you prioritized these themes"
- "What surprised you in your research?"
- "If you had two more weeks, what would you explore further?"
- "How would you measure success in the first quarter?"
- "What's the biggest risk in this strategy?"

---

## Subtype 2: Success Metrics / KPIs

**Question patterns:** "What metrics would you set for Uber Eats?," "How would you measure success for X?"
**Source videos:** `vpsUw1vP8w4`, `dz-hqOih9qY`

### Dimension Weights

| Dimension | Weight |
|-----------|--------|
| Process Rigor | 15% |
| Metrics Quality | 35% |
| Data-Driven Thinking | 20% |
| Prioritization & Trade-offs | 20% |
| Communication Clarity | 10% |

Metrics Quality is dominant — this is fundamentally a metrics question.

### Scoring Scale

#### 2. Metrics Quality (35%)

| Score | Description |
|-------|-------------|
| 5 | Follows 5-step framework (`vpsUw1vP8w4`): understand product → qualitative goals → quantitative metrics → 2-3 North Star metrics → counter-metrics + downstream. For marketplace products, selects metrics that capture value for ALL sides (e.g., "number of nights booked and stayed" captures host, guest, and platform value, `dz-hqOih9qY`). Explains WHY each metric matters for the specific product — not generic DAU/MAU. Explicitly rejects metrics that don't fit with reasoning (e.g., retention for infrequent-use products). |
| 4 | Strong North Star with counter-metrics. Metrics specific to the product. Justification for choices. |
| 3 | Relevant primary and secondary metrics with basic rationale. |
| 2 | Lists metrics without prioritization or specificity. Generic DAU/MAU/revenue. |
| 1 | Cannot identify relevant metrics or picks vanity metrics. |

**Key signal from videos:** "Make it specific to the product you're talking about, not just throwing out generic metrics that show memorization versus thinking" (`vpsUw1vP8w4`). "This isn't high school — you're not going to impress them by memorizing things" (`vpsUw1vP8w4`).

#### 3. Data-Driven Thinking (20%)

| Score | Description |
|-------|-------------|
| 5 | Understands metric relationships (leading/lagging, input/output). Identifies how counter-metrics protect against gaming. Proposes experimentation approach for validating metric choices. |
| 4 | Good metric relationships with counter-metrics. |
| 3 | Some understanding of metric relationships. |
| 2 | Metrics listed independently without understanding interactions. |
| 1 | No understanding of how metrics relate. |

#### 4. Prioritization & Trade-offs (20%)

| Score | Description |
|-------|-------------|
| 5 | Articulates trade-offs between competing metrics. Identifies when optimizing for one metric could hurt another. Proposes guardrail metrics to prevent unintended consequences. |
| 4 | Clear trade-off awareness with guardrails. |
| 3 | Some trade-off awareness. |
| 2 | No trade-off consideration. |
| 1 | Metrics contradict each other without awareness. |

### Common Mistakes (Metrics)

- Listing generic metrics (DAU, MAU, time spent) without explaining why they matter for this product (`vpsUw1vP8w4`)
- Showing memorization of frameworks (AARRR pirates) rather than thoughtful analysis (`vpsUw1vP8w4`)
- Choosing retention for infrequent-use products without acknowledging measurement challenges (`dz-hqOih9qY`)
- Using ratings as a metric without noting they're not representative — "only extreme experiences get rated" (`dz-hqOih9qY`)
- Not considering all sides of a marketplace when selecting metrics (`dz-hqOih9qY`)

### Follow-up Probes

- "This metric dropped 20% week-over-week. Walk me through your investigation."
- "How would you distinguish between correlation and causation here?"
- "What's the counter-metric to your North Star?"
- "If you could only track one metric, which would it be and why?"

---

## Subtype 3: Root Cause / Debugging

**Question patterns:** "Uber riders dropped 20% — what's going on?," "YouTube views are down — investigate"
**Source videos:** `vpsUw1vP8w4`

### Dimension Weights

| Dimension | Weight |
|-----------|--------|
| Process Rigor | 25% |
| Metrics Quality | 15% |
| Data-Driven Thinking | 30% |
| Prioritization & Trade-offs | 20% |
| Communication Clarity | 10% |

Data-Driven Thinking is dominant — systematic hypothesis generation and validation is the core skill.

### Scoring Scale

#### 1. Process Rigor (25%)

| Score | Description |
|-------|-------------|
| 5 | Follows 6-step debugging framework (`vpsUw1vP8w4`): 1) Clarifying questions (timespan, geography, scope), 2) Understand the product, 3) Map the user flow, 4) Generate hypotheses, 5) Explain validation approach, 6) Propose solutions. Thinks through the problem systematically rather than going through a memorized checklist. |
| 4 | Clear systematic approach. Asks good clarifying questions. Maps user flow before hypothesizing. |
| 3 | Reasonable structure but may skip clarifying questions or jump to hypotheses too quickly. |
| 2 | Some structure but unsystematic. Guesses at causes. |
| 1 | No systematic approach. Guesses a single cause. |

#### 3. Data-Driven Thinking (30%)

| Score | Description |
|-------|-------------|
| 5 | Generates comprehensive hypothesis tree. Segments by internal vs. external, time-based, user-type-based. Considers seasonality and confounding factors. For each hypothesis, proposes specific data to validate/invalidate. Identifies non-obvious causes. |
| 4 | Strong hypothesis generation with validation steps. Good segmentation of potential causes. |
| 3 | 3-5 reasonable hypotheses with basic validation approach. |
| 2 | 1-2 hypotheses without systematic exploration. |
| 1 | Single guess with no validation plan. |

#### 4. Prioritization & Trade-offs (20%)

| Score | Description |
|-------|-------------|
| 5 | Prioritizes hypotheses by likelihood and impact. Proposes solutions for top hypotheses with trade-off analysis. Considers which fixes are reversible vs. permanent. |
| 4 | Good hypothesis prioritization with solution proposals. |
| 3 | Some prioritization of hypotheses. |
| 2 | Treats all hypotheses equally. |
| 1 | No prioritization. |

### Common Mistakes (Debugging)

- Going through a memorized checklist rather than thinking through the problem (`vpsUw1vP8w4`)
- Not asking clarifying questions about timeframe, geography, scope
- Jumping to a single cause without systematic exploration
- Not mapping the user flow before generating hypotheses
- Proposing solutions without validating the root cause first

### Follow-up Probes

- "What data would you look at first?"
- "How would you distinguish between a bug and a market change?"
- "How confident are you in this hypothesis? What would change your mind?"
- "How would you communicate this investigation to leadership?"
- "How would you set up an A/B test to validate your fix?"
